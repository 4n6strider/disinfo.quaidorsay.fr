
# Characterizing the fight against disinformation

A temporal approach of the fight against disinformation, with moments of reaction to disinformation thought as phases (similar to those of an epidemic), allows for an overview gathering three main, overlapping processes: prevention, research and treatment of disinformation.

## Prevention

In the face of false, misleading and/or harmful information, some actors fighting disinformation dedicate themselves to **sensitization** endeavours, among which can be found **media literacy** initiatives aimed at the civil society, or local **proactive measures** (prevention campaigns, deterrent laws, report mechanisms, etc.), for most of which efficiency remains to be assessed.

## Research

A field of **academic research** regarding disinformation and its elimination (if not eradication) has progressively developed these last few years (notably in social sciences and security studies), especially after the acknowledgement of the potential impact of disinformation campaigns on the electoral process and social stability of many a democracy. Empirical observations and analyses are part of the fight against disinformation as they allow for a better, mutualized understanding of issues and concepts related to disinformation.

## Treatment

Various actors operate within the context of the fight against disinformation, each using its own tools and practices. We however distinguish **three key-steps in the treatment process of information**: its **Detection**, its **Qualification** as disinformation and lastly the **Reaction** to this kind of content. 
Potentially, an **Attribution** phase can be added after the Qualification phase, which would consist in the definition of the origin of treated disinformation.

Each actor determines the **threshold** after which content goes from the Detection phase to the Qualification phase. This threshold is delimited according to the **resources** an actor has and is willing to allocate towards the treatment of an information. The bigger the resources, the higher the **reactivity** of the actor to this information.  The threshold thus depends on the "importance" of the information as measured by the agent, _i.e._ **according to specific criteria**.

By "Qualification", one means the **analysis** of information according to such criteria, which allows its definition and **categorization**.  

**Criteria** used to Qualify detected content are defined independently by each actor. Of course, **core criteria to define disinformation**, such as falsehood, are common to all. However, more **specific** criteria can be **as various as the ones defining them**. For instance, a Foreign Affairs Ministry will not have the same criteria as an Interior Ministry, for whom the "foreign" dimension of an issue is of minor importance, if of any. The same goes for other agents, from NGOs to social scientists or analysts, who will  focus on specific criteria of information depending on their range of action. Therefore, these criteria narrow the **scope of Reaction** of the agents who define them.

As a team working within the French Ministry of Foreign Affairs and dealing with disinformation issues, we chose to limit ourselves to five main criteria, which are the **massiveness** of the diffusion of a content, the **artificiality** of its propagation and/or of those who spread it, the **foreignness** of its origin (author, country, state or domain external to the state where the disinformation is spread), the **intent to harm** it bears and the **verifiable falsehood or misleading nature** of information.

These criteria  come from the June 2018 [EU Commission Communication](http://ec.europa.eu/information_society/newsroom/image/document/2018-28/presentationcomm_paolo_cesarini_202D869F-9A13-6D79-FC46C00EAAE3E9AC_53429.pdf)  and the [EEAS Action plan against disinformation](https://eeas.europa.eu/sites/eeas/files/action_plan_against_disinformation.pdf). However, on some points slight changes were made. For instance, the EEAS includes internal as well as external disinformation efforts. The qualifications used by the EEAS of "deliberate" and "large-scale" echo our "intent to harm" and "massiveness" criteria. Massiveness also echoes the word "systematic" used in the report refers to both coordination and a significant power scale. It is intimately linked to "our" criterion of "artificiality". In the EEAS communication, artificiality is also directly addressed as a major part of disinformation since it accounts for the use of bots, fake accounts and video manipulation, among other tools and techniques.
